<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>X Wallet Liveliness</title>
  
  <!-- Responsive Meta Tag for Mobile/Tablet -->
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <!-- <meta name="viewport" content="width=device-width, initial-scale=1.0, viewport-fit=cover"> -->

  <script>
    (function() {
      const urlParams = new URLSearchParams(window.location.search);
      const id = urlParams.get('id');
      // Regular expression for a UUID (v4/v1)
      const uuidRegex = /^[0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12}$/i;
      
      // Check if the id exists and if it matches the UUID format
      if (!id || !uuidRegex.test(id)) {
        alert("Access denied!");
        window.location.href = "about:blank";
      }
    })();
  </script>

  <!-- jQuery -->
  <script src="https://code.jquery.com/jquery-3.3.1.min.js"></script> 

  <!-- Font Awesome -->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">

  <!-- TensorFlow.js Dependencies -->
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-core@2.6.0/dist/tf-core.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-converter@2.6.0/dist/tf-converter.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-backend-wasm@2.6.0/dist/tf-backend-wasm.min.js"></script>

  <!-- Face Landmarks Detection Model -->
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/face-landmarks-detection@0.0.2/dist/face-landmarks-detection.min.js"></script>

  <!-- Webcam Easy -->
  <script src="https://unpkg.com/webcam-easy/dist/webcam-easy.min.js"></script>

  <style>
    /* Use flex layout to center content on mobile/tablet devices */
    body {
      margin: 0;
      padding: 0;
      display: flex;
      flex-direction: column;
      justify-content: center;
      align-items: center;
      min-height: 100vh;
      background-color: #f0f0f0;
      font-family: Arial, sans-serif;
    }

    h1 {
      margin-bottom: 20px;
      text-align: center;
    }

    #webcam-control {
      margin: 20px;
      text-align: center;
    }

    .btn {
      cursor: pointer;
      background-color: #4c4f4c;
      border: none;
      padding: 15px;
      color: white;
      font-size: 16px;
      border-radius: 5px;
      margin: 10px;
    }

    /* 
      Responsive container for video and canvases:
      - The container maintains a 4:3 aspect ratio.
      - It has a maximum width of 640px, preserving the 640Ã—480 coordinate system.
    */
    #video-container {
      position: relative;
      width: 100%;
      max-width: 640px;
      /* Height will be set dynamically to maintain a 4:3 aspect ratio */
      margin: 0 auto;
      display: none; /* Shown after the webcam starts */
      border: 2px solid #4CAF50;
      border-radius: 5px;
    }

    /* Video and canvas elements scale to fill the container */
    #webcam,
    #bounding-box,
    #facial-landmarks {
      position: absolute;
      top: 0;
      left: 0;
      width: 100%;
      height: 100%;
      object-fit: cover;
    }

    /* Prevent pointer events on overlay canvases */
    .overlay-canvas {
      pointer-events: none;
    }

    #info-text {
      display: block;
      margin-top: 10px;
      font-weight: bold;
      text-align: center;
    }

    #download-photo {
      display: none;
      margin-top: 10px;
      font-size: 16px;
      color: #4CAF50;
      text-decoration: none;
    }
  </style>
</head>
<body>

  <h1>XWallet Liveliness</h1>

  <div id="webcam-control">
    <input type="checkbox" id="webcam-switch">
    <span id="webcam-caption">Initiate Liveliness Now!</span>
  </div>

  <div id="video-container">
    <video id="webcam" autoplay playsinline width="640" height="480"></video>
    <canvas id="bounding-box" class="overlay-canvas" width="640" height="480"></canvas>
    <canvas id="facial-landmarks" class="overlay-canvas" width="640" height="480"></canvas>

    <!-- Controls positioned at the bottom of the container -->
    <div style="position: absolute; bottom: 10px; width: 100%; text-align: center;">
      <button id="start-capture" class="btn">
        <i class="fa fa-camera fa-fw"></i> Begin Liveliness
      </button>
      <!-- <button id="capture-running" class="btn" style="display: none;">
        <i class="fa fa-refresh fa-spin fa-fw"></i> Capturing...
      </button> -->
      <span id="info-text" style="display: none;">Please blink to capture.</span>

      <div style="margin-top: 10px;">
        <a id="download-photo" href="#" download="snapshot.png">Download Snapshot</a>
      </div>
    </div>
  </div>

  <script>
    // --- Dynamic Resize Logic ---
    // function adjustVideoSize() {
    //   const container = document.getElementById('video-container');
    //   // Use 90% of the window's width or 640px (whichever is smaller)
    //   const width = Math.min(window.innerWidth * 0.9, 640);
    //   // Maintain a 4:3 aspect ratio (height = width * 3/4)
    //   container.style.width = width + 'px';
    //   container.style.height = (width * 3 / 4) + 'px';
    // }


    function adjustVideoSize() {
  const container = document.getElementById('video-container');
  // Use 90% of the window's width or 640px (whichever is smaller)
  const width = Math.min(window.innerWidth * 0.9, 640);
  // Maintain a 4:3 aspect ratio
  const height = width * 3 / 4;
  
  // Update the container size
  container.style.width = width + 'px';
  container.style.height = height + 'px';

  // Update the video element intrinsic dimensions
  const video = document.getElementById('webcam');
  video.setAttribute('width', width);
  video.setAttribute('height', height);

  // Update the bounding box canvas intrinsic dimensions
  const boundingBoxCanvas = document.getElementById('bounding-box');
  boundingBoxCanvas.setAttribute('width', width);
  boundingBoxCanvas.setAttribute('height', height);

  // Update the facial landmarks canvas intrinsic dimensions
  const landmarksCanvas = document.getElementById('facial-landmarks');
  landmarksCanvas.setAttribute('width', width);
  landmarksCanvas.setAttribute('height', height);
}

    
    // Adjust size on load, resize, and orientation change
    window.addEventListener('resize', adjustVideoSize);
    window.addEventListener('orientationchange', adjustVideoSize);
    document.addEventListener('DOMContentLoaded', adjustVideoSize);

    // --- Webcam and Face Detection Logic ---
    const webcamElement = document.getElementById('webcam');
    const boundingBoxCanvas = document.getElementById('bounding-box');
    const boundingBoxCtx = boundingBoxCanvas.getContext('2d');
    const landmarksCanvas = document.getElementById('facial-landmarks');
    const landmarksCtx = landmarksCanvas.getContext('2d');
    // Initialize Webcam Easy; you can also pass custom constraints if needed
    const webcam = new Webcam(webcamElement, 'user', landmarksCanvas);

    const emojiMapping = {
      blink: 'ðŸ‘€',
      smile: 'ðŸ˜Š',
      left: 'â¬…ï¸',
      right: 'âž¡ï¸',
      up: 'â¬†ï¸',
      down: 'â¬‡ï¸',
      'up-left': 'â†–ï¸',
      'up-right': 'â†—ï¸',
      'down-left': 'â†™ï¸',
      'down-right': 'â†˜ï¸'
    };

    let model = null;
    let cameraFrame = null;
    let running = false;
    let timeout = null;
    
    // Total time allowed for the liveliness test (in seconds)
    const TOTAL_TIME = 30;
    let remainingTime = TOTAL_TIME;
    let countdownInterval = null;
    
    // Application states: "waitingForBlink" â†’ "waitingForHeadMovements" â†’ "waitingForSmile" â†’ "completed"
    let currentStep = 'waitingForBlink';

    // All possible head movement directions
    const headMovements = ['left', 'right', 'up', 'down', 'up-left', 'up-right', 'down-left', 'down-right'];
    // The sequence of exactly 5 head movements
    let headMovementSequence = [];
    let currentHeadMovementIndex = 0;

    // Instead of forcing the user to center their face, we capture the initial nose position once a face is detected.
    let initialNosePosition = null;

    // DOM Elements
    const startCaptureBtn = $("#start-capture");
    // const captureRunningBtn = $("#capture-running"); // (unused in this version)
    const webcamSwitch = $("#webcam-switch");
    const webcamCaption = $("#webcam-caption");
    const infoText = $("#info-text");
    const downloadPhotoLink = $("#download-photo");

    // Event Listeners
    startCaptureBtn.click(function () {
      startCapture();
    });

    webcamSwitch.change(function () {
      if (this.checked) {
        startWebcam();
      } else {      
        stopCapture();
        webcam.stop();
        webcamCaption.text('Click to Start Camera');
        $("#video-container").hide();
        console.log("Webcam stopped");
      }        
    });

    // Start Webcam Function
    async function startWebcam() {
      try {
        await webcam.start();
        webcamCaption.text('Click to Stop Camera');
        $("#video-container").show();
        console.log("Webcam started");
      } catch (err) {
        console.error("Error starting webcam:", err);
        alert("Could not access the webcam. Please allow camera access.");
        webcamSwitch.prop('checked', false);
      }
    }

    // Stop Capture Function
    async function stopCapture() {
      if (running) {
        // Reset UI elements
        startCaptureBtn.show();
        infoText.hide();
        running = false;
        if (cameraFrame != null) {
          cancelAnimationFrame(cameraFrame);
        }
        // Clear the canvases
        boundingBoxCtx.clearRect(0, 0, boundingBoxCanvas.width, boundingBoxCanvas.height);
        landmarksCtx.clearRect(0, 0, landmarksCanvas.width, landmarksCanvas.height);
      }
      // Clear any active countdown timer
      clearInterval(countdownInterval);
      remainingTime = TOTAL_TIME;
      // Reset the initial nose position for next capture
      initialNosePosition = null;
    }

    // Start Capture Function
    async function startCapture() {
      // UI updates
      // captureRunningBtn.show();  // (if you wish to show a â€œcapturingâ€ indicator)
      startCaptureBtn.hide();
      infoText.text('Please blink to capture.');
      infoText.show();
      currentStep = 'waitingForBlink';
      // Reset baseline and head movement sequence
      initialNosePosition = null;
      headMovementSequence = [];
      currentHeadMovementIndex = 0;
      downloadPhotoLink.hide();

      try {
        // Load the MediaPipe Facemesh model
        model = await faceLandmarksDetection.load(
          faceLandmarksDetection.SupportedPackages.mediapipeFacemesh,
          { maxFaces: 1 }
        );
        running = true;
        cameraFrame = requestAnimationFrame(detectKeyPoints);

        // Set a fallback timeout (in case no blink is detected)
        timeout = setTimeout(() => {
          alert("Capture timed out. Please try again.");
          stopCapture();
          webcam.stop();
        }, TOTAL_TIME * 1000);                

      } catch (err) {
        console.error("Error loading the model:", err);
        alert("Failed to load the face detection model.");
        stopCapture();
      }
    }

    // Get the current instruction text (for head movement, blink, smile)
    function getCurrentInstruction() {
      switch (currentStep) {
        case 'waitingForBlink':
          return `Please blink ${emojiMapping.blink}`;
        case 'waitingForHeadMovements':
          const movement = headMovementSequence[currentHeadMovementIndex];
          return movement
            ? `Please move your head ${movement} ${emojiMapping[movement]}`
            : 'Please move your head.';
        case 'waitingForSmile':
          return `Please smile ${emojiMapping.smile}`;
        default:
          return "";
      }
    }

    // Initialize TensorFlow.js Backend
    async function setupFaceLandmarkDetection() {
      try {
        await tf.setBackend('wasm');
        await tf.ready();
        console.log("TensorFlow.js backend 'wasm' is ready.");
      } catch (err) {
        console.error("Error setting up TensorFlow.js backend:", err);
        alert("Failed to initialize TensorFlow.js backend.");
      }
    }

    // Main detection loop â€“ note that we removed any check that forces the face to be centered.
    async function detectKeyPoints() {
      if (!running) return;

      const predictions = await model.estimateFaces({
        input: webcamElement,
        returnTensors: false,
        flipHorizontal: true,
        predictIrises: false
      });

      // Clear previous drawings
      boundingBoxCtx.clearRect(0, 0, boundingBoxCanvas.width, boundingBoxCanvas.height);
      landmarksCtx.clearRect(0, 0, landmarksCanvas.width, landmarksCanvas.height);

      if (predictions.length > 0) {
        const prediction = predictions[0];
        const keypoints = prediction.scaledMesh;

        // Draw the bounding box on the detected face
        const start = prediction.boundingBox.topLeft;
        const end = prediction.boundingBox.bottomRight;
        const size = [end[0] - start[0], end[1] - start[1]];
        boundingBoxCtx.strokeStyle = "blue";
        boundingBoxCtx.lineWidth = 2;
        boundingBoxCtx.strokeRect(start[0], start[1], size[0], size[1]);

        // Instead of asking the user to center the face, we capture the initial nose position
        // once when in the 'waitingForBlink' step.
        if (currentStep === 'waitingForBlink' && initialNosePosition === null) {
          // Using the nose tip (keypoints index 1) as a baseline reference.
          initialNosePosition = { x: keypoints[1][0], y: keypoints[1][1] };
          console.log("Initial nose position set:", initialNosePosition);
        }

        // Draw facial landmarks (for feedback)
        drawFacialLandmarks(keypoints, currentStep);

        // If in the head movement step, display the arrow indicator based on the initial nose position.
        if (currentStep === 'waitingForHeadMovements') {
          const currentDirection = headMovementSequence[currentHeadMovementIndex];
          drawHeadIndicator(currentDirection);
        }

        // Process current state
        switch (currentStep) {
          case 'waitingForBlink':
            if (detectBlink(keypoints)) {
              console.log('Blink detected!');
              // Generate a sequence of 5 random head movements
              for (let i = 0; i < 5; i++) {
                headMovementSequence.push(
                  headMovements[Math.floor(Math.random() * headMovements.length)]
                );
              }
              console.log("Head Movement Sequence:", headMovementSequence);
              currentStep = 'waitingForHeadMovements';

              // Start the 30-second countdown
              remainingTime = TOTAL_TIME;
              countdownInterval = setInterval(() => {
                remainingTime--;
                infoText.text(
                  `Time remaining: ${remainingTime} seconds. ${getCurrentInstruction()}`
                );
                if (remainingTime <= 0) {
                  clearInterval(countdownInterval);
                  alert("Capture timed out. Please try again.");
                  stopCapture();
                  webcam.stop();
                }
              }, 1000);

              infoText.text(
                `Time remaining: ${remainingTime} seconds. ${getCurrentInstruction()}`
              );
            }
            break;

          case 'waitingForHeadMovements': {
              const currentDirection = headMovementSequence[currentHeadMovementIndex];
              if (detectHeadMovement(keypoints, currentDirection)) {
                console.log(`Head movement (${currentDirection}) detected!`);
                currentHeadMovementIndex++;
                if (currentHeadMovementIndex < headMovementSequence.length) {
                  infoText.text(
                    `Time remaining: ${remainingTime} seconds. Good! Next, please move your head ${headMovementSequence[currentHeadMovementIndex]}.`
                  );
                } else {
                  infoText.text(
                    `Time remaining: ${remainingTime} seconds. All head movements detected! Now, please smile.`
                  );
                  currentStep = 'waitingForSmile';
                }
              }
            }
            break;

          case 'waitingForSmile':
            if (detectSmile(keypoints)) {
              console.log('Smile detected!');
              infoText.text('Smile detected! Capture completed.');
              clearTimeout(timeout);
              clearInterval(countdownInterval);
              stopCapture();
              let picture = webcam.snap();
              downloadPhotoLink.attr('href', picture);
              downloadPhotoLink.show();
              infoText.text('Capture completed! You can download your snapshot.');
              currentStep = 'completed';
              webcam.stop();
              webcamElement.style.display = "none";
              console.log("Webcam closed after snapshot capture.");

              // Send the snapshot to the mobile container (React Native WebView)
              const urlParams = new URLSearchParams(window.location.search);
              const id = urlParams.get('id');
              const uuidRegex = /^[0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12}$/i;
              if (!id || !uuidRegex.test(id)) {
                alert("Access denied!");
              } else {
                const payload = { snapshot: picture, id: id };
                if (window.ReactNativeWebView && typeof window.ReactNativeWebView.postMessage === 'function') {
                  window.ReactNativeWebView.postMessage(JSON.stringify(payload));
                } else {
                  console.warn("Not running inside a ReactNativeWebView. This page is optimized for mobile/tablet use.");
                }
              }
            }
            break;
          default:
            break;
        }
      } else {
        console.log("No face detected.");
      }

      cameraFrame = requestAnimationFrame(detectKeyPoints);
    }

    // Update head movement detection to use the initial nose position as baseline.
    function detectHeadMovement(keypoints, direction) {
      const noseTipIndex = 1;
      const nose = keypoints[noseTipIndex];
      const noseX = nose[0];
      const noseY = nose[1];
      // Use the initial nose position if available; otherwise fall back to the canvas center.
      let baselineX = landmarksCanvas.width / 2;
      let baselineY = landmarksCanvas.height / 2;
      if (initialNosePosition) {
        baselineX = initialNosePosition.x;
        baselineY = initialNosePosition.y;
      }
      const THRESHOLD_X = 30;
      const THRESHOLD_Y = 20;
      let moved = false;
      
      switch(direction) {
        case 'left':
          if (noseX < baselineX - THRESHOLD_X) moved = true;
          break;
        case 'right':
          if (noseX > baselineX + THRESHOLD_X) moved = true;
          break;
        case 'up':
          if (noseY < baselineY - THRESHOLD_Y) moved = true;
          break;
        case 'down':
          if (noseY > baselineY + THRESHOLD_Y) moved = true;
          break;
        case 'up-left':
          if (noseX < baselineX - THRESHOLD_X && noseY < baselineY - THRESHOLD_Y) moved = true;
          break;
        case 'up-right':
          if (noseX > baselineX + THRESHOLD_X && noseY < baselineY - THRESHOLD_Y) moved = true;
          break;
        case 'down-left':
          if (noseX < baselineX - THRESHOLD_X && noseY > baselineY + THRESHOLD_Y) moved = true;
          break;
        case 'down-right':
          if (noseX > baselineX + THRESHOLD_X && noseY > baselineY + THRESHOLD_Y) moved = true;
          break;
        default:
          break;
      }
      
      console.log(
        `Nose: (${noseX.toFixed(1)}, ${noseY.toFixed(1)}) | Baseline: (${baselineX.toFixed(1)}, ${baselineY.toFixed(1)}) | Target: ${direction} | Moved: ${moved}`
      );
      return moved;
    }

    // Update the head movement indicator so that its base is the initial nose position.
    function drawHeadIndicator(direction) {
      landmarksCtx.save();
      landmarksCtx.strokeStyle = "yellow";
      landmarksCtx.fillStyle = "yellow";
      landmarksCtx.lineWidth = 4;
      
      const baseX = initialNosePosition ? initialNosePosition.x : landmarksCanvas.width / 2;
      const baseY = initialNosePosition ? initialNosePosition.y : landmarksCanvas.height / 2;
      const arrowLength = 40;
      
      landmarksCtx.beginPath();
      switch(direction) {
        case 'left':
          landmarksCtx.moveTo(baseX, baseY);
          landmarksCtx.lineTo(baseX - arrowLength, baseY);
          landmarksCtx.lineTo(baseX - arrowLength + 10, baseY - 10);
          landmarksCtx.moveTo(baseX - arrowLength, baseY);
          landmarksCtx.lineTo(baseX - arrowLength + 10, baseY + 10);
          break;
        case 'right':
          landmarksCtx.moveTo(baseX, baseY);
          landmarksCtx.lineTo(baseX + arrowLength, baseY);
          landmarksCtx.lineTo(baseX + arrowLength - 10, baseY - 10);
          landmarksCtx.moveTo(baseX + arrowLength, baseY);
          landmarksCtx.lineTo(baseX + arrowLength - 10, baseY + 10);
          break;
        case 'up':
          landmarksCtx.moveTo(baseX, baseY);
          landmarksCtx.lineTo(baseX, baseY - arrowLength);
          landmarksCtx.lineTo(baseX - 10, baseY - arrowLength + 10);
          landmarksCtx.moveTo(baseX, baseY - arrowLength);
          landmarksCtx.lineTo(baseX + 10, baseY - arrowLength + 10);
          break;
        case 'down':
          landmarksCtx.moveTo(baseX, baseY);
          landmarksCtx.lineTo(baseX, baseY + arrowLength);
          landmarksCtx.lineTo(baseX - 10, baseY + arrowLength - 10);
          landmarksCtx.moveTo(baseX, baseY + arrowLength);
          landmarksCtx.lineTo(baseX + 10, baseY + arrowLength - 10);
          break;
        case 'up-left':
          landmarksCtx.moveTo(baseX, baseY);
          landmarksCtx.lineTo(baseX - arrowLength, baseY - arrowLength);
          landmarksCtx.lineTo(baseX - arrowLength + 10, baseY - arrowLength + 10);
          landmarksCtx.moveTo(baseX - arrowLength, baseY - arrowLength);
          landmarksCtx.lineTo(baseX - arrowLength + 10, baseY - arrowLength - 10);
          break;
        case 'up-right':
          landmarksCtx.moveTo(baseX, baseY);
          landmarksCtx.lineTo(baseX + arrowLength, baseY - arrowLength);
          landmarksCtx.lineTo(baseX + arrowLength - 10, baseY - arrowLength + 10);
          landmarksCtx.moveTo(baseX + arrowLength, baseY - arrowLength);
          landmarksCtx.lineTo(baseX + arrowLength - 10, baseY - arrowLength - 10);
          break;
        case 'down-left':
          landmarksCtx.moveTo(baseX, baseY);
          landmarksCtx.lineTo(baseX - arrowLength, baseY + arrowLength);
          landmarksCtx.lineTo(baseX - arrowLength + 10, baseY + arrowLength - 10);
          landmarksCtx.moveTo(baseX - arrowLength, baseY + arrowLength);
          landmarksCtx.lineTo(baseX - arrowLength + 10, baseY + arrowLength + 10);
          break;
        case 'down-right':
          landmarksCtx.moveTo(baseX, baseY);
          landmarksCtx.lineTo(baseX + arrowLength, baseY + arrowLength);
          landmarksCtx.lineTo(baseX + arrowLength - 10, baseY + arrowLength - 10);
          landmarksCtx.moveTo(baseX + arrowLength, baseY + arrowLength);
          landmarksCtx.lineTo(baseX + arrowLength - 10, baseY + arrowLength + 10);
          break;
        default:
          break;
      }
      landmarksCtx.stroke();
      landmarksCtx.restore();
    }

    // Blink Detection
    let blinkFrameCount = 0;
    function detectBlink(keypoints) {
      const leftEyeIndices = { top: 386, bottom: 374, left: 263, right: 362 };
      const rightEyeIndices = { top: 159, bottom: 145, left: 33, right: 133 };
      const leftEAR = calculateEAR(keypoints, leftEyeIndices);
      const rightEAR = calculateEAR(keypoints, rightEyeIndices);
      console.log(`Left EAR: ${leftEAR.toFixed(3)}\tRight EAR: ${rightEAR.toFixed(3)}`);
      
      const EAR_THRESHOLD = 0.5;
      const REQUIRED_CONSECUTIVE_FRAMES = 3;
      
      if (leftEAR < EAR_THRESHOLD && rightEAR < EAR_THRESHOLD) {
        blinkFrameCount++;
      } else {
        blinkFrameCount = 0;
      }
      
      if (blinkFrameCount >= REQUIRED_CONSECUTIVE_FRAMES) {
        blinkFrameCount = 0;
        return true;
      }
      
      return false;
    }

    // Smile Detection
    function detectSmile(keypoints) {
      const mouth = { left: 61, right: 291, top: 13, bottom: 14 };
      const horizontalDist = euclideanDistance(
        keypoints[mouth.left][0], keypoints[mouth.left][1],
        keypoints[mouth.right][0], keypoints[mouth.right][1]
      );
      const verticalDist = euclideanDistance(
        keypoints[mouth.top][0], keypoints[mouth.top][1],
        keypoints[mouth.bottom][0], keypoints[mouth.bottom][1]
      );
      const mar = verticalDist / horizontalDist;
      console.log(`Mouth Aspect Ratio (MAR): ${mar.toFixed(3)}`);
      const MAR_THRESHOLD = 0.16;
      return mar > MAR_THRESHOLD;
    }

    // Draw Facial Landmarks
    function drawFacialLandmarks(keypoints, step) {
      landmarksCtx.lineWidth = 2;
      landmarksCtx.strokeStyle = "white";
      landmarksCtx.fillStyle = "white";

      const eyes = {
        left: [33, 7, 163, 144, 145, 153, 154, 155, 133, 33],
        right: [263, 249, 390, 373, 374, 380, 381, 382, 362, 263]
      };
      const mouth = {
        outer: [
          61, 146, 91, 181, 84, 17, 314, 405, 321, 375, 
          291, 308, 324, 318, 402, 317, 14, 87, 178, 88, 
          95, 185, 40, 39, 37, 0, 267, 269, 270, 409, 
          415, 310, 311, 312, 13, 82, 81, 80, 191, 78, 95
        ]
      };

      function drawLines(landmarkIndices) {
        landmarksCtx.beginPath();
        for (let i = 0; i < landmarkIndices.length; i++) {
          const [x, y] = keypoints[landmarkIndices[i]];
          if (i === 0) {
            landmarksCtx.moveTo(x, y);
          } else {
            landmarksCtx.lineTo(x, y);
          }
        }
        landmarksCtx.stroke();
      }

      // Draw eyes
      drawLines(eyes.left);
      drawLines(eyes.right);

      // Optionally, during head movement steps, you can also draw an outline
      // (this code remains unchanged)
      if (step === 'waitingForHeadMovements') {
        const faceOutline = [
          10, 338, 297, 332, 284, 251, 389, 356, 454, 323, 
          361, 288, 397, 365, 379, 378, 400, 377, 152, 148, 
          176, 149, 150, 136, 172, 58, 132, 93, 234, 127, 
          162, 21, 54, 103, 67, 109, 10
        ];
        drawLines(faceOutline);
      }

      // Draw mouth when waiting for smile
      if (step === 'waitingForSmile') {
        drawLines(mouth.outer);
      }
    }

    // Calculate Eye Aspect Ratio (EAR)
    function calculateEAR(keypoints, eyeIndices) {
      const a = euclideanDistance(
        keypoints[eyeIndices.top][0], keypoints[eyeIndices.top][1],
        keypoints[eyeIndices.bottom][0], keypoints[eyeIndices.bottom][1]
      );
      const b = euclideanDistance(
        keypoints[eyeIndices.left][0], keypoints[eyeIndices.left][1],
        keypoints[eyeIndices.right][0], keypoints[eyeIndices.right][1]
      );
      return a / (2.0 * b);
    }

    // Euclidean Distance
    function euclideanDistance(x1, y1, x2, y2) {
      return Math.sqrt(Math.pow((x1 - x2), 2) + Math.pow((y1 - y2), 2));
    }

    // Initialize the application
    async function main() {
      await setupFaceLandmarkDetection();
    }
    main();
  </script>

</body>
</html>
